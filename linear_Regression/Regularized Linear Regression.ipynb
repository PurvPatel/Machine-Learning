{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4879bf78",
   "metadata": {},
   "source": [
    "# A Simple Implementation of Regularized Linear Regression\n",
    "This notebook goes over a implementation of Regularized Linear Regression in Python. The purpose of this notebook is to mainly break down how the linear regression algorithm/model works both through a bare bones implementation and visualization.\n",
    "\n",
    "First, let's define the key formulas we will need to implement for our model to function:\n",
    "### The Hypothesis Function\n",
    "This function will actually be used to predict our outputs ($y$) value based on a given set of inputs ($x$). In this case, our $h(x)$ is going to be a linear function of $n$ dimensional inputs; $\\{x_{1}, ..., x_{n}\\}$, along with $\\{\\theta_{0}, ..., \\theta_{n}\\}$ trainable parameters:\n",
    "\n",
    "$h_{\\theta}(x)  =  \\theta_{0} + \\theta_{1} * x_{1} + ... + \\theta_{n} * x_{n}  =  \\theta_{0} + \\sum \\limits _{i=1} ^{n} \\theta_{i}x_{i}$\n",
    "\n",
    "### The Cost Function\n",
    "This function will be used to assess how well our model can predict the output based on the input $x$ for each labelled data point $\\{x, y\\}$. We will be using the mean squared error to calculate the cost of each prediction. The $m$ value is the number of training points. Both $x^{(i)}$ and $\\theta$ are vectors of size $n\\times1$. The second term is the regularization term. When the cost function is being optimized, it will aide in stopping the model from overfitting the training parameters onto the training set. $\\lambda$ will control how much weight we want to give the regularization term, this means if $\\lambda$ is set too large, the model might have a high bias and perform poorly. Note, $\\theta_{0}$ is not regularized.\n",
    "\n",
    "$J(\\theta)  =  \\frac{1}{2m}  \\sum \\limits _{i=1} ^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} + \\frac{\\lambda}{2m}\\sum \\limits _{j=1} ^{n} \\theta_{j}^{2}$\n",
    "\n",
    "### The Gradient Calculation Function\n",
    "The optimization of the cost function will be done with gradient descent. For this, the gradients of all the trainable parameters must be calculated, that is, the partial derivitives of $\\{\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}, ..., \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}}\\}$. After calculating the partiel derivatives, the result should be:\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}  =  \\frac{1}{m}  \\sum \\limits _{i=1} ^{m} {(h_{\\theta}(x^{(i)}) - y^{(i)})}$\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}  =  \\frac{1}{m}  \\sum \\limits _{i=1} ^{m} {(h_{\\theta}(x^{(i)}) - y^{(i)}) x_{j} ^{i}  +  \\frac{\\lambda}{m} \\theta_{j}}   \\hspace{10mm} \\text{for} \\hspace{3mm}j = 1, ..., n$\n",
    "\n",
    "With these calulations, the gradient descent algorithm can be used to optimize the cost function and train the model. Here is a implementation of Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1de894ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RegularizedLinearRegression:\n",
    "    def __init__(self, n=1, reg_lambda=0.01, learning_rate=0.001, epsilon=1e-9):\n",
    "        self.n = n\n",
    "        self.theta = np.zeros((n + 1, 1))\n",
    "        self.grad = np.zeros((n + 1, 1))\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon\n",
    "        \n",
    "    def train(self, X, y, iterations=None):\n",
    "        \n",
    "        # get number of training examples and insert m by 1 bias vector\n",
    "        m = X.shape[0]\n",
    "        X = np.insert(X, [0], np.ones((m, 1)), axis=1)\n",
    "        \n",
    "        cost_hist = np.zeros(iterations)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            # run batch gradient descent step and calculate cost of updated theta\n",
    "            self._gradient_descent_batch_step(X, y, m)\n",
    "            cost_hist[i] = self._cost_func(X, y)\n",
    "            \n",
    "        return cost_hist\n",
    "            \n",
    "    def _gradient_descent_batch_step(self, X, y, m):\n",
    "\n",
    "        # calculate gradients of theta (calculated grad of theta_0 again to remove the regularization calculation from the vectoried cal.)\n",
    "        self.grad = (1/m) * np.transpose(np.matmul(np.transpose(np.sum(np.subtract(self._hypothesis_func(X), y), axis=1)), X)) + (self.reg_lambda/m) * self.theta \n",
    "        self.grad[0] = (1/m) * np.sum(np.subtract(self._hypothesis_func(X), y))\n",
    "        \n",
    "        # update theta\n",
    "        self.theta = self.theta - self.learning_rate * self.grad\n",
    "        \n",
    "    def _hypothesis_func(self, X):\n",
    "        # hypothesis function for prediction\n",
    "        return np.matmul(X, self.theta)\n",
    "    \n",
    "    def _cost_func(self, X, y):\n",
    "        # mean squared error cost\n",
    "        m = X.shape[0]\n",
    "        return 1/(2 * m) * (np.sum(np.power(np.subtract(self._hypothesis_func(X), y), 2))  +  self.reg_lambda * np.sum(np.power(self.theta[1:], 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
