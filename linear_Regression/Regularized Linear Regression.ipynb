{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4879bf78",
   "metadata": {},
   "source": [
    "# A Simple Implementation of Regularized Linear Regression\n",
    "This notebook goes over a implementation of Regularized Linear Regression in Python. The purpose of this notebook is to mainly break down how the linear regression algorithm/model works both through a bare bones implementation and visualization.\n",
    "\n",
    "First, let's define the key formulas we will need to implement for our model to function:\n",
    "### The Hypothesis Function\n",
    "This function will actually be used to predict our outputs ($y$) value based on a given set of inputs ($x$). In this case, our $h(x)$ is going to be a linear function of $n$ dimensional inputs; $\\{x_{1}, ..., x_{n}\\}$, along with $\\{\\theta_{0}, ..., \\theta_{n}\\}$ trainable parameters:\n",
    "\n",
    "$h_{\\theta}(x)  =  \\theta_{0} + \\theta_{1} * x_{1} + ... + \\theta_{n} * x_{n}  =  \\theta_{0} + \\sum_{i=1}^{n} \\theta_{i}x_{i}$\n",
    "\n",
    "### The Cost Function\n",
    "This function will be used to assess how well our model can predict the output based on the input $x$ for each labelled data point $\\{x, y\\}$. We will be using the mean squared error to calculate the cost of each prediction. The $m$ value is the number of training points. Both $x^{(i)}$ and $\\theta$ are vectors of size $n\\times1$. The second term is the regularization term. When the cost function is being optimized, it will aide in stopping the model from overfitting the training parameters onto the training set. $\\lambda$ will control how much weight we want to give the regularization term, this means if $\\lambda$ is set too large, the model might have a high bias and perform poorly. Note, $\\theta_{0}$ is not regularized.\n",
    "\n",
    "$J(\\theta)  =  \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} + \\frac{\\lambda}{2m}\\sum_{j=1}^{n} \\theta_{j}^{2}$\n",
    "\n",
    "### The Gradient Calculation Function\n",
    "The optimization of the cost function will be done with gradient descent. For this, the gradients of all the trainable parameters must be calculated, that is, the partial derivitives of $\\{\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}, ..., \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}}\\}$. After calculating the partiel derivatives, the result should be:\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}  =  \\frac{1}{m}  \\sum_{i=1}^{m} {(h_{\\theta}(x^{(i)}) - y^{(i)})}$\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}  =  \\frac{1}{m}  \\sum_{i=1}^{m} {(h_{\\theta}(x^{(i)}) - y^{(i)}) x_{j} ^{i}  +  \\frac{\\lambda}{m} \\theta_{j}}   \\hspace{10mm} \\text{for} \\hspace{3mm}j = 1, ..., n$\n",
    "\n",
    "With these calulations, the gradient descent algorithm can be used to optimize the cost function and train the model. Here is a vectorized implementation of Regularized Linear Regression. The training set is decoupled from the actual regression trainer to make it easier to switch between batch, mini-batch, and stochastic gradient descent. Input data X is an m by n matrix and input label y is a m by 1 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb4a9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RegularizedLinearRegression:\n",
    "    def __init__(self, n=1, reg_lambda=0.01, learning_rate=0.001):\n",
    "        self.n = n\n",
    "        self.theta = np.zeros((n + 1, 1))\n",
    "        self.grad = np.zeros((n + 1, 1))\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self, X, y, iterations=None):\n",
    "        \n",
    "        # get number of training examples and insert m by 1 bias vector\n",
    "        m = X.shape[0]\n",
    "        X = np.insert(X, [0], np.ones((m, 1)), axis=1)\n",
    "        \n",
    "        cost_hist = np.zeros(iterations)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            # run batch gradient descent step and calculate cost of updated theta\n",
    "            self._gradient_descent_batch_step(X, y)\n",
    "            cost_hist[i] = self._cost_func(X, y)\n",
    "            \n",
    "        return cost_hist\n",
    "            \n",
    "    def _gradient_descent_batch_step(self, X, y):\n",
    "\n",
    "        # compute gradients\n",
    "        self.grad = self._compute_gradients(X, y)\n",
    "        \n",
    "        # update theta\n",
    "        self.theta = self.theta - self.learning_rate * self.grad\n",
    "        \n",
    "    def _hypothesis_func(self, X, theta=None):\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "            \n",
    "        # hypothesis function for prediction\n",
    "        return np.matmul(X, theta)\n",
    "    \n",
    "    def _cost_func(self, X, y, theta=None):\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "            \n",
    "        # mean squared error cost\n",
    "        m = X.shape[0]\n",
    "        return 1/(2 * m) * (np.sum(np.power(np.subtract(self._hypothesis_func(X, theta), y), 2))  +  self.reg_lambda * np.sum(np.power(theta[1:], 2)))\n",
    "        \n",
    "    def _compute_gradients(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        grad = np.zeros((self.n + 1, 1))\n",
    "        \n",
    "        # calculate gradients of theta (calculated grad of theta_0 again to remove the regularization calculation from the vectoried cal.)\n",
    "        grad = (1/m) * np.transpose(np.matmul(np.transpose(np.sum(np.subtract(self._hypothesis_func(X), y), axis=1, keepdims=True)), X)) + (self.reg_lambda/m) * self.theta \n",
    "        grad[0] = (1/m) * np.sum(np.subtract(self._hypothesis_func(X), y))\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249de5ae",
   "metadata": {},
   "source": [
    "### Gradient Checking\n",
    "Now that gradient descent has been implementated, there needs to be some way to check that the gradient calculations are in fact accurate and there is no logic error present. To do this, gradient checking must be implemented. Gradient checking follows the basic principles of computing tangents of functions both through limits and derivatives and then comparing the two outputs to make sure they are consistent with one another.\n",
    "\n",
    "Here is a simple example with a polynomial function ($n=1$). \n",
    "\n",
    "Polynomial function\n",
    "\n",
    "$f(x) = x^2$\n",
    "\n",
    "Tangent (derivative) Calculation througn limits ($h$ being a small value; $h < 0.01$ )\n",
    "\n",
    "$f'(x) = \\frac{f(x + h) - f(x - h)}{2 * h}$\n",
    "\n",
    "Tangent (derivative) Calculation througn derivative\n",
    "\n",
    "$f'(x) = 2x$\n",
    "\n",
    "Now, sample random values of $x$, choose a small $h$ value, and compare the derivative outputs given by the two methods to confirm that both methods are working correctly. You should get similiar values from both methods.\n",
    "\n",
    "This concept can be applied to the cost function and gradients of the linear regression model. The only slight change is that  the cost function contains more than one parameter; $\\{\\theta_{0}, ..., \\theta_{n}\\}$. To overcome this, change only one parameter at a time. $h$ will now be a (n + 1) by 1 vector of zeros, with only one element at a time having a slight deviation. Then, cycle through each element of $\\{\\theta_{0}, ..., \\theta_{n}\\}$ to calculate every gradient using the limit method. Below is a modified Regularized Linear Regression with gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1de894ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RegularizedLinearRegression:\n",
    "    def __init__(self, n=1, reg_lambda=0.01, learning_rate=0.001):\n",
    "        self.n = n\n",
    "        self.theta = np.zeros((n + 1, 1))\n",
    "        self.grad = np.zeros((n + 1, 1))\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self, X, y, iterations=None):\n",
    "        \n",
    "        # get number of training examples and insert m by 1 bias vector\n",
    "        m = X.shape[0]\n",
    "        X = np.insert(X, [0], np.ones((m, 1)), axis=1)\n",
    "        \n",
    "        cost_hist = np.zeros(iterations)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            # run batch gradient descent step and calculate cost of updated theta\n",
    "            self._gradient_descent_batch_step(X, y)\n",
    "            cost_hist[i] = self._cost_func(X, y)\n",
    "            \n",
    "        return cost_hist\n",
    "    \n",
    "    def check_gradients(self, sample_size=10, epsilon=1e-3):\n",
    "        \n",
    "        # generate random samples\n",
    "        X_samples = np.random.rand(sample_size, self.n)\n",
    "        X_samples = np.insert(X_samples, [0], np.ones((X_samples.shape[0], 1)), axis=1)\n",
    "        y_samples = np.random.rand(sample_size, 1)\n",
    "        \n",
    "        #calculate gradients manually through limits and through the gradient equations\n",
    "        num_grad = self._compute_numerical_gradients(X_samples, y_samples, epsilon)\n",
    "        grad = self._compute_gradients(X_samples, y_samples)\n",
    "        \n",
    "        # calculate average error between the two gradients\n",
    "        average_error = (1/sample_size) * np.abs(num_grad - grad)\n",
    "        \n",
    "        # return calculated gradients and the average error\n",
    "        return np.insert(grad, [0], num_grad, axis=1), average_error\n",
    "            \n",
    "    def _gradient_descent_batch_step(self, X, y):\n",
    "\n",
    "        # compute gradients\n",
    "        self.grad = self._compute_gradients(X, y)\n",
    "        \n",
    "        # update theta\n",
    "        self.theta = self.theta - self.learning_rate * self.grad\n",
    "        \n",
    "    def _hypothesis_func(self, X, theta=None):\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "            \n",
    "        # hypothesis function for prediction\n",
    "        return np.matmul(X, theta)\n",
    "    \n",
    "    def _cost_func(self, X, y, theta=None):\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "            \n",
    "        # mean squared error cost\n",
    "        m = X.shape[0]\n",
    "        return 1/(2 * m) * (np.sum(np.power(np.subtract(self._hypothesis_func(X, theta), y), 2))  +  self.reg_lambda * np.sum(np.power(theta[1:], 2)))\n",
    "        \n",
    "    def _compute_gradients(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        grad = np.zeros((self.n + 1, 1))\n",
    "        \n",
    "        # calculate gradients of theta (calculated grad of theta_0 again to remove the regularization calculation from the vectoried cal.)\n",
    "        grad = (1/m) * np.transpose(np.matmul(np.transpose(np.sum(np.subtract(self._hypothesis_func(X), y), axis=1, keepdims=True)), X)) + (self.reg_lambda/m) * self.theta \n",
    "        grad[0] = (1/m) * np.sum(np.subtract(self._hypothesis_func(X), y))\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def _compute_numerical_gradients(self, X, y, epsilon):\n",
    "        \n",
    "        # declare gradient vector and perturbation vec; this vec contains slight deviations for one theta param at a time\n",
    "        num_grad = np.zeros((self.n + 1, 1))\n",
    "        perturbation_vec = np.zeros((self.n + 1, 1))\n",
    "        \n",
    "        # calculate numerical gradients using cost func\n",
    "        for i,_ in enumerate(self.theta):\n",
    "            perturbation_vec[i] = epsilon\n",
    "            cost_1 = self._cost_func(X, y, np.add(self.theta, perturbation_vec))\n",
    "            cost_2 = self._cost_func(X, y, np.subtract(self.theta, perturbation_vec))\n",
    "            num_grad[i] = (cost_1 - cost_2) / (2 * epsilon)\n",
    "            perturbation_vec[i] = 0\n",
    "            \n",
    "        return num_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b552801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "m = 50\n",
    "X = np.random.rand(m, n)\n",
    "y = np.random.rand(m, 1)\n",
    "RLR = RegularizedLinearRegression(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9b2dbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 300\n",
    "cost_hist = RLR.train(X, y, iterations=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "54611fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(1, iterations + 1), cost_hist)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Training Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
