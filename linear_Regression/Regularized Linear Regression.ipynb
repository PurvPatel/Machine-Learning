{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4879bf78",
   "metadata": {},
   "source": [
    "# A Simple Implementation of Regularized Linear Regression\n",
    "This notebook goes over a implementation of Regularized Linear Regression in Python. The purpose of this notebook is to mainly break down how the linear regression algorithm/model works both through a bare bones implementation and visualization.\n",
    "\n",
    "First, let's define the key formulas we will need to implement for our model to function:\n",
    "### The Hypothesis Function\n",
    "This function will actually be used to predict our outputs ($y$) value based on a given set of inputs ($x$). In this case, our $h(x)$ is going to be a linear function of $n$ dimensional inputs; $\\{x_{1}, ..., x_{n}\\}$, along with $\\{\\theta_{0}, ..., \\theta_{n}\\}$ trainable parameters:\n",
    "\n",
    "$h_{\\theta}(x)  =  \\theta_{0} + \\theta_{1} * x_{1} + ... + \\theta_{n} * x_{n}  =  \\theta_{0} + \\sum_{i=1}^{n} \\theta_{i}x_{i}$\n",
    "\n",
    "### The Cost Function\n",
    "This function will be used to assess how well our model can predict the output based on the input $x$ for each labelled data point $\\{x, y\\}$. We will be using the mean squared error to calculate the cost of each prediction. The $m$ value is the number of training points. Both $x^{(i)}$ and $\\theta$ are vectors of size $n\\times1$. The second term is the regularization term. When the cost function is being optimized, it will aide in stopping the model from overfitting the training parameters onto the training set. $\\lambda$ will control how much weight we want to give the regularization term, this means if $\\lambda$ is set too large, the model might have a high bias and perform poorly. Note, $\\theta_{0}$ is not regularized.\n",
    "\n",
    "$J(\\theta)  =  \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} + \\frac{\\lambda}{2m}\\sum_{j=1}^{n} \\theta_{j}^{2}$\n",
    "\n",
    "### The Gradient Calculation Function\n",
    "The optimization of the cost function will be done with gradient descent. For this, the gradients of all the trainable parameters must be calculated, that is, the partial derivitives of $\\{\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}, ..., \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}}\\}$. After calculating the partiel derivatives, the result should be:\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}  =  \\frac{1}{m}  \\sum_{i=1}^{m} {(h_{\\theta}(x^{(i)}) - y^{(i)})}$\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}  =  \\frac{1}{m}  \\sum_{i=1}^{m} {(h_{\\theta}(x^{(i)}) - y^{(i)}) x_{j} ^{i}  +  \\frac{\\lambda}{m} \\theta_{j}}   \\hspace{10mm} \\text{for} \\hspace{3mm}j = 1, ..., n$\n",
    "\n",
    "With these calulations, the gradient descent algorithm can be used to optimize the cost function and train the model. Here is a vectorized implementation of Regularized Linear Regression. The training set is decoupled from the actual regression trainer to make it easier to switch between batch, mini-batch, and stochastic gradient descent. Input data X is an m by n matrix and input label y is a m by 1 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70ec77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RegularizedLinearRegression:\n",
    "    def __init__(self, n=1, reg_lambda=0.01, learning_rate=0.001):\n",
    "        self.n = n\n",
    "        self.theta = np.zeros((n + 1, 1))\n",
    "        self.grad = np.zeros((n + 1, 1))\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self, X, y, iterations=None):\n",
    "        \n",
    "        # get number of training examples and insert m by 1 bias vector\n",
    "        m = X.shape[0]\n",
    "        X = np.insert(X, [0], np.ones((m, 1)), axis=1)\n",
    "        \n",
    "        cost_hist = np.zeros(iterations)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            # run batch gradient descent step and calculate cost of updated theta\n",
    "            self._gradient_descent_batch_step(X, y)\n",
    "            cost_hist[i] = self._cost_func(X, y)\n",
    "            \n",
    "        return cost_hist\n",
    "            \n",
    "    def _gradient_descent_batch_step(self, X, y):\n",
    "\n",
    "        # compute gradients\n",
    "        self.grad = self._compute_gradients(X, y)\n",
    "        \n",
    "        # update theta\n",
    "        self.theta = self.theta - self.learning_rate * self.grad\n",
    "        \n",
    "    def _hypothesis_func(self, X, theta=None):\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "            \n",
    "        # hypothesis function for prediction\n",
    "        return np.matmul(X, theta)\n",
    "    \n",
    "    def _cost_func(self, X, y, theta=None):\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "            \n",
    "        # mean squared error cost\n",
    "        m = X.shape[0]\n",
    "        return 1/(2 * m) * (np.sum(np.power(np.subtract(self._hypothesis_func(X, theta), y), 2))  +  self.reg_lambda * np.sum(np.power(theta[1:], 2)))\n",
    "        \n",
    "    def _compute_gradients(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        grad = np.zeros((self.n + 1, 1))\n",
    "        \n",
    "        # calculate gradients of theta (calculated grad of theta_0 again to remove the regularization calculation from the vectoried cal.)\n",
    "        grad = (1/m) * np.transpose(np.matmul(np.transpose(np.sum(np.subtract(self._hypothesis_func(X), y), axis=1, keepdims=True)), X)) + (self.reg_lambda/m) * self.theta \n",
    "        grad[0] = (1/m) * np.sum(np.subtract(self._hypothesis_func(X), y))\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0bd68",
   "metadata": {},
   "source": [
    "### Gradient Checking\n",
    "Now that gradient descent has been implementated, there needs to be some way to check that the gradient calculations are in fact accurate and there is no logic error present. To do this, gradient checking can be implemented. Gradient checking follows the basic principles of computing tangents of functions both through limits and derivatives and then comparing the two outputs to make sure they are consistent with one another.\n",
    "\n",
    "Here is a simple example with a polynomial function ($n=1$). \n",
    "\n",
    "Polynomial function\n",
    "\n",
    "$f(x) = x^2$\n",
    "\n",
    "Tangent (derivative) Calculation througn limits ($h$ being a small value; $h < 0.01$ )\n",
    "\n",
    "$f'(x) = \\frac{f(x + h) - f(x - h)}{2 * h}$\n",
    "\n",
    "Tangent (derivative) Calculation througn derivative\n",
    "\n",
    "$f'(x) = 2x$\n",
    "\n",
    "Now, sample random values of $x$, choose a small $h$ value, and compare the derivative outputs given by the two methods to confirm that both methods are working correctly. You should get similiar values from both methods.\n",
    "\n",
    "This concept can be applied to the cost function and gradients (derivatives of the cost function) of the linear regression model. The only slight change is that  the cost function contains more than one parameter; $\\{\\theta_{0}, ..., \\theta_{n}\\}$. To overcome this, change only one parameter at a time. $h$ will now be a (n + 1) by 1 vector of zeros, with only one element at a time having a slight deviation. Then, cycle through each element of $\\{\\theta_{0}, ..., \\theta_{n}\\}$ to calculate every gradient using the limit method. Below is a modified Regularized Linear Regression with gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de894ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RegularizedLinearRegression:\n",
    "    def __init__(self, n=1, reg_lambda=0.01, learning_rate=0.001):\n",
    "        self.n = n\n",
    "        self.theta = np.zeros((n + 1, 1))\n",
    "        self.grad = np.zeros((n + 1, 1))\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self, X, y, iterations=None):\n",
    "        \n",
    "        # get number of training examples and insert m by 1 bias vector\n",
    "        m = X.shape[0]\n",
    "        X = np.insert(X, [0], np.ones((m, 1)), axis=1)\n",
    "        \n",
    "        cost_hist = np.zeros(iterations)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            # run batch gradient descent step and calculate cost of updated theta\n",
    "            self._gradient_descent_batch_step(X, y)\n",
    "            cost_hist[i] = self._cost_func(X, y)\n",
    "            \n",
    "        return cost_hist\n",
    "    \n",
    "    def check_gradients(self, sample_size=10, epsilon=1e-3):\n",
    "        \n",
    "        # generate random samples\n",
    "        X_samples = np.random.rand(sample_size, self.n)\n",
    "        X_samples = np.insert(X_samples, [0], np.ones((X_samples.shape[0], 1)), axis=1)\n",
    "        y_samples = np.random.rand(sample_size, 1)\n",
    "        \n",
    "        #calculate gradients manually through limits and through the gradient equations\n",
    "        num_grad = self._compute_numerical_gradients(X_samples, y_samples, epsilon)\n",
    "        grad = self._compute_gradients(X_samples, y_samples)\n",
    "        \n",
    "        # calculate average error between the two gradients\n",
    "        average_error =  np.abs(num_grad - grad).mean()\n",
    "        \n",
    "        # return calculated gradients and the average error\n",
    "        return np.insert(grad, [0], num_grad, axis=1), average_error\n",
    "            \n",
    "    def _gradient_descent_batch_step(self, X, y):\n",
    "\n",
    "        # compute gradients\n",
    "        self.grad = self._compute_gradients(X, y)\n",
    "\n",
    "        # update theta\n",
    "        self.theta = self.theta - self.learning_rate * self.grad\n",
    "        \n",
    "    def _hypothesis_func(self, X, theta=None):\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "            \n",
    "        # hypothesis function for prediction\n",
    "        return np.matmul(X, theta)\n",
    "    \n",
    "    def _cost_func(self, X, y, theta=None):\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "            \n",
    "        # mean squared error cost\n",
    "        m = X.shape[0]\n",
    "        return 1/(2 * m) * (np.sum(np.power(np.subtract(self._hypothesis_func(X, theta), y), 2))  +  self.reg_lambda * np.sum(np.power(theta[1:], 2)))\n",
    "        \n",
    "    def _compute_gradients(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        grad = np.zeros((self.n + 1, 1))\n",
    "        \n",
    "        # calculate gradients of theta (calculated grad of theta_0 again to remove the regularization calculation from the vectoried cal.)\n",
    "        grad = (1/m) * np.transpose(np.matmul(np.transpose(np.sum(np.subtract(self._hypothesis_func(X), y), axis=1, keepdims=True)), X)) + (self.reg_lambda/m) * self.theta \n",
    "        grad[0] = (1/m) * np.sum(np.subtract(self._hypothesis_func(X), y))\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def _compute_numerical_gradients(self, X, y, epsilon):\n",
    "        \n",
    "        # declare gradient vector and perturbation vec; this vec contains slight deviations for one theta param at a time\n",
    "        num_grad = np.zeros((self.n + 1, 1))\n",
    "        perturbation_vec = np.zeros((self.n + 1, 1))\n",
    "        \n",
    "        # calculate numerical gradients using cost func\n",
    "        for i,_ in enumerate(self.theta):\n",
    "            perturbation_vec[i] = epsilon\n",
    "            cost_1 = self._cost_func(X, y, np.add(self.theta, perturbation_vec))\n",
    "            cost_2 = self._cost_func(X, y, np.subtract(self.theta, perturbation_vec))\n",
    "            num_grad[i] = (cost_1 - cost_2) / (2 * epsilon)\n",
    "            perturbation_vec[i] = 0\n",
    "            \n",
    "        return num_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7ee10",
   "metadata": {},
   "source": [
    "Below is sample run with 50 random examples, each with 5 features. Notice how when even the y value is generated randomly, the cost goes down everytime, this is because our cost function is convex, meaning there is a global minimum for it to converage to. This sample run does not really give any meaningful insight into the functionality of linear regression, so I will demenstrate this algorithm on a real dataset next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b552801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "m = 50\n",
    "X = np.random.rand(m, n)\n",
    "y = np.random.rand(m, 1)\n",
    "RLR = RegularizedLinearRegression(n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a1aaf",
   "metadata": {},
   "source": [
    "Before training, let's test that our gradients are functioning correctly using gradient checking with a sample size of 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2bb33f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.640606334916659e-15\n"
     ]
    }
   ],
   "source": [
    "_, average_error = RLR.check_gradients(15)\n",
    "print(average_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45fb523",
   "metadata": {},
   "source": [
    "Running it a few times tell me that linear regresssion is functioning correctly as the average error is pretty small; $< 10^{-13}$. Let's train the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2dbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 300\n",
    "cost_hist = RLR.train(X, y, iterations=iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7aa4c",
   "metadata": {},
   "source": [
    "Let's plot our cost history next to see that the model was properly trained and is converging to some minimum. If the learning is too large, it might diverge, making the model useless for prediction. A downward sloping curve should be seen, since this was trained on random distributions with no real relations between the output and input, there will be no noticable drop in the beginning iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4851a968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(1, iterations + 1), cost_hist)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d57f6d",
   "metadata": {},
   "source": [
    "Next, let's try out linear regression with an actual dataset that has some correlation with the input and output. I am going to use the california housing dataset provided by sklearn. I am also going to use pandas just to load the data and get a feel for how it looks and what it is made up of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e1f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11fa84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_housing = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d83835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0368</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.761658</td>\n",
       "      <td>1.103627</td>\n",
       "      <td>413.0</td>\n",
       "      <td>2.139896</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>2.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.6591</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.931907</td>\n",
       "      <td>0.951362</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>2.128405</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>2.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.1200</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.797527</td>\n",
       "      <td>1.061824</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1.788253</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>2.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0804</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.294118</td>\n",
       "      <td>1.117647</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>2.026891</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>2.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.6912</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.970588</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>2.172269</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>2.611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "5  4.0368      52.0  4.761658   1.103627       413.0  2.139896     37.85   \n",
       "6  3.6591      52.0  4.931907   0.951362      1094.0  2.128405     37.84   \n",
       "7  3.1200      52.0  4.797527   1.061824      1157.0  1.788253     37.84   \n",
       "8  2.0804      42.0  4.294118   1.117647      1206.0  2.026891     37.84   \n",
       "9  3.6912      52.0  4.970588   0.990196      1551.0  2.172269     37.84   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  \n",
       "5    -122.25        2.697  \n",
       "6    -122.25        2.992  \n",
       "7    -122.25        2.414  \n",
       "8    -122.26        2.267  \n",
       "9    -122.25        2.611  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cali_housing.data.shape)\n",
    "cali_housing.frame.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3efc6",
   "metadata": {},
   "source": [
    "As seen above, there are 20640 samples present in total and it looks like we have 9 real value descriptions for each sample. It should also be noted that each sample contains data average over a city block. A normal business insight we could gleem from this data is can I predict the value of a house in california based on the features of the block it is located on? Well, let's see. I will first seperate the data into training, cross validation, and test sets so there is a way to measure the performance of a model. The target output value to predict will be the Median House Value. Let's fetch input and output as numpy arrays first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba1715f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_housing_X, cali_housing_y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "393296d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Input Vector Shape:  (20640, 8)\n",
      "y Output Vector Shape: (20640,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X Input Vector Shape:  {}\".format(cali_housing_X.shape))\n",
    "print(\"y Output Vector Shape: {}\".format(cali_housing_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175549d",
   "metadata": {},
   "source": [
    "Notice how the y output vector does not explicity state the second parameter of its shape. This is a rank 1 numpy array and it can lead to weird results during matrix calculations. Let's reshape that into a 20640 by 1 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a63f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y Output Vector Shape: (20640, 1)\n"
     ]
    }
   ],
   "source": [
    "cali_housing_y = np.reshape(cali_housing_y, (20640, 1))\n",
    "print(\"y Output Vector Shape: {}\".format(cali_housing_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7503450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's shuffle our data first to have a uniform spread among all three subsets\n",
    "perm = np.random.permutation(cali_housing_X.shape[0])\n",
    "\n",
    "X_shuffled = cali_housing_X[perm]\n",
    "y_shuffled = cali_housing_y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe7f9b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = cali_housing_X.shape[0]\n",
    "train_ind = int(total_size * 0.7)\n",
    "val_ind = train_ind + int(total_size * 0.1)\n",
    "\n",
    "X_train, y_train = X_shuffled[0:train_ind], y_shuffled[0:train_ind] \n",
    "X_val, y_val = X_shuffled[train_ind:val_ind], y_shuffled[train_ind:val_ind]\n",
    "X_test, y_test = X_shuffled[val_ind:], y_shuffled[val_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53bae11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:   X = (14447, 8), y = (14447, 1)\n",
      "Validation set size: X = (2064, 8), y = (2064, 1)\n",
      "Testing set size:    X = (4129, 8), y = (4129, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:   X = {}, y = {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Validation set size: X = {}, y = {}\".format(X_val.shape, y_val.shape))\n",
    "print(\"Testing set size:    X = {}, y = {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81407d69",
   "metadata": {},
   "source": [
    "Linear Regression has to also be updated to track both the training and validation loss over each iteration. Here is the updated code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b2faad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RegularizedLinearRegression:\n",
    "    def __init__(self, n=1, reg_lambda=0.01, learning_rate=0.001):\n",
    "        self.n = n\n",
    "        self.theta = np.zeros((n + 1, 1))\n",
    "        self.grad = np.zeros((n + 1, 1))\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self, X, y, iterations, X_val=None, y_val=None):\n",
    "        \n",
    "        # get number of training examples and insert m by 1 bias vector\n",
    "        m = X.shape[0]\n",
    "        X = np.insert(X, [0], np.ones((m, 1)), axis=1)\n",
    "        \n",
    "        training_cost_hist = np.zeros(iterations)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            validation_cost_hist = np.zeros(iterations)\n",
    "            X_val = np.insert(X_val, [0], np.ones((X_val.shape[0], 1)), axis=1)\n",
    "            val = True\n",
    "        else:\n",
    "            val=False\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            # run batch gradient descent step and calculate cost of updated theta\n",
    "            self._gradient_descent_batch_step(X, y)\n",
    "            training_cost_hist[i] = self._cost_func(X, y)\n",
    "            \n",
    "            if val is not None:\n",
    "                validation_cost_hist[i] = self._cost_func(X_val, y_val)\n",
    "            \n",
    "        if val is not None:\n",
    "            return training_cost_hist, validation_cost_hist\n",
    "        return training_cost_hist\n",
    "    \n",
    "    def test(self, X_test, y_test):\n",
    "        X_test = np.insert(X_test, [0], np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        return self._cost_func(X_test, y_test)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.insert(X, [0], np.ones((X.shape[0], 1)), axis=1)\n",
    "        return self._hypothesis_func(X)\n",
    "    \n",
    "    def check_gradients(self, sample_size=10, epsilon=1e-3):\n",
    "        \n",
    "        # generate random samples\n",
    "        X_samples = np.random.rand(sample_size, self.n)\n",
    "        X_samples = np.insert(X_samples, [0], np.ones((X_samples.shape[0], 1)), axis=1)\n",
    "        y_samples = np.random.rand(sample_size, 1)\n",
    "        \n",
    "        #calculate gradients manually through limits and through the gradient equations\n",
    "        num_grad = self._compute_numerical_gradients(X_samples, y_samples, epsilon)\n",
    "        grad = self._compute_gradients(X_samples, y_samples)\n",
    "        \n",
    "        # calculate average error between the two gradients\n",
    "        average_error =  np.abs(num_grad - grad).mean()\n",
    "        \n",
    "        # return calculated gradients and the average error\n",
    "        return np.insert(grad, [0], num_grad, axis=1), average_error\n",
    "            \n",
    "    def _gradient_descent_batch_step(self, X, y):\n",
    "\n",
    "        # compute gradients\n",
    "        self.grad = self._compute_gradients(X, y)\n",
    "        \n",
    "        # update theta\n",
    "        self.theta = self.theta - self.learning_rate * self.grad\n",
    "        \n",
    "    def _hypothesis_func(self, X, theta=None):\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "            \n",
    "        # hypothesis function for prediction\n",
    "        return np.matmul(X, theta)\n",
    "    \n",
    "    def _cost_func(self, X, y, theta=None):\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "            \n",
    "        # mean squared error cost\n",
    "        m = X.shape[0]\n",
    "        return 1/(2 * m) * (np.sum(np.power(np.subtract(self._hypothesis_func(X, theta), y), 2))  +  self.reg_lambda * np.sum(np.power(theta[1:], 2)))\n",
    "        \n",
    "    def _compute_gradients(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        grad = np.zeros((self.n + 1, 1))\n",
    "        \n",
    "        # calculate gradients of theta (calculated grad of theta_0 again to remove the regularization calculation from the vectoried cal.)\n",
    "        grad = (1/m) * np.transpose(np.matmul(np.transpose(np.sum(np.subtract(self._hypothesis_func(X), y), axis=1, keepdims=True)), X)) + (self.reg_lambda/m) * self.theta \n",
    "        grad[0] = (1/m) * np.sum(np.subtract(self._hypothesis_func(X), y))\n",
    "        return grad\n",
    "    \n",
    "    def _compute_numerical_gradients(self, X, y, epsilon):\n",
    "        \n",
    "        # declare gradient vector and perturbation vec; this vec contains slight deviations for one theta param at a time\n",
    "        num_grad = np.zeros((self.n + 1, 1))\n",
    "        perturbation_vec = np.zeros((self.n + 1, 1))\n",
    "        \n",
    "        # calculate numerical gradients using cost func\n",
    "        for i,_ in enumerate(self.theta):\n",
    "            perturbation_vec[i] = epsilon\n",
    "            cost_1 = self._cost_func(X, y, np.add(self.theta, perturbation_vec))\n",
    "            cost_2 = self._cost_func(X, y, np.subtract(self.theta, perturbation_vec))\n",
    "            num_grad[i] = (cost_1 - cost_2) / (2 * epsilon)\n",
    "            perturbation_vec[i] = 0\n",
    "            \n",
    "        return num_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dfc3fd",
   "metadata": {},
   "source": [
    "For the first model being trained, I am not going to normalize the features. This means that I am not going to make sure all the features are in the same range so the model might give preference to training parameters assciated with larger feature values since opitmizing them might give a much better cost. I had to severely reduce the learning rate to keep the gradients from diverging due to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eb659e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train.shape[1]\n",
    "iterations = 100000\n",
    "cali_housing_price_predictor = RegularizedLinearRegression(n=n, learning_rate=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ec7371b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_cost_hist, val_cost_hist = cali_housing_price_predictor.train(X_train, y_train, iterations, X_val, y_val)\n",
    "test_loss = cali_housing_price_predictor.test(X_test,y_test)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(1, iterations + 1), train_cost_hist, label=\"training_loss\")\n",
    "plt.plot(np.arange(1, iterations + 1), val_cost_hist, label=\"validation_loss\")\n",
    "plt.axhline(y=test_loss, label=\"test_loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d8ef",
   "metadata": {},
   "source": [
    "Above is a sample training loop with 100000 iterations. I've tried it with iterations={200, 500, 1000, 10000, 100000} and reshuffled the data a couple of times along with varying the hyperparameters. Each time, the test error is actually relatively low on the graph. This might be due to the fact that train, test, and val sets are very similiar, because this is pretty odd to see. Anyways, the optimial low error is around 0.64 which is not that great considering the y range is (0 - 5). All these target house values are also in $10^{5}$ dollars with a lot of them being $< 10^{4}$, meaning our squared means error is pretty high realistically. There might also be problems with the hand implementation of Linear Regression; I was not taking things such as numerical computation into account when implementing it (defining proper data types and so forth). This is mainly a demo, so I will not be delving into that here though. I will normalize the features and retrain the model to see how it performs next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
